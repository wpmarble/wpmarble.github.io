<!DOCTYPE html>

<html lang="en" data-theme=""><head>
    <title> William Marble | Deep Learning and Knowledge Representation </title>
    <meta charset="utf-8"><meta name="generator" content="Hugo 0.134.2"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
    <meta name="description" content="Political Scientist&lt;br&gt;University of Pennsylvania">
    
    <link rel="stylesheet" href="https://williammarble.com/css/style.min.9d767deb7194ace9a7de9d6c25497deeaa306cac523a8082fd9ef84c765e4ca8.css" integrity="sha256-nXZ963GUrOmn3p1sJUl97qowbKxSOoCC/Z74THZeTKg=" crossorigin="anonymous" type="text/css">
    <link rel="stylesheet" href="https://williammarble.com/css/custom.7cb4a26810608d0f516b73a18d8cbbb4bedb43da17e0df15ae90d8aaab763cb7.css" integrity="sha256-fLSiaBBgjQ9Ra3OhjYy7tL7bQ9oX4N8VrpDYqqt2PLc=" crossorigin="anonymous" type="text/css">
    <link rel="stylesheet" href="https://williammarble.com/css/academicons.min.56051c2496053a55f5fc0c8b494c0af42c35324ad95a74e45235a9d511386ebf.css" integrity="sha256-VgUcJJYFOlX1/AyLSUwK9Cw1MkrZWnTkUjWp1RE4br8=" crossorigin="anonymous" type="text/css">
    <link rel="stylesheet" href="https://williammarble.com/css/academicons.64dbc5d3e37b416ac0df4073806f85fee0b920aa58e6bb601599bafcbea8a8ac.css" integrity="sha256-ZNvF0&#43;N7QWrA30BzgG&#43;F/uC5IKpY5rtgFZm6/L6oqKw=" crossorigin="anonymous" type="text/css">
   <link rel="stylesheet" 
    href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css" 
    integrity="sha512-+4zCK9k+qNFUR5X+cKL9EIR+ZOhtIloNl9GIKS57V1MyNsYpYcUrUeQc9vNfzsWfV28IaLL3i96P9sdNyeRssA==" 
    crossorigin="anonymous" />

    
    <link rel="shortcut icon" href="https://williammarble.com/favicons/favicon.ico" type="image/x-icon">
    <link rel="apple-touch-icon" sizes="180x180" href="https://williammarble.com/favicons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://williammarble.com/favicons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://williammarble.com/favicons/favicon-16x16.png">
    <link rel="canonical" href="https://williammarble.com/posts/2022-04-05-deep-learning-knowledge-representation/">
    
    
    <script type="text/javascript" src="https://williammarble.com/js/anatole-header.min.a3fa728a9f57833a31dfb45c48caaf1e4890c8c97f07bd7133fc2359745edb5d.js" integrity="sha256-o/pyip9Xgzox37RcSMqvHkiQyMl/B71xM/wjWXRe210=" crossorigin="anonymous"></script>
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Deep Learning and Knowledge Representation">
  <meta name="twitter:description" content="In “Deep Learning is Hitting A Wall,” Gary Marcus argues that deep learning is reaching its limits as a paradigm for artificial intelligence. Recent difficulties with self-driving cars and unreliable language model outputs illustrate the limitations of systems that cannot understand the world, or the meaning of the words they are parroting. Instead, Marcus advocates that field of AI turn its attention to symbolic approaches. Such approaches would focus on encoding information about the world and deriving information using simple operations:
What does “manipulating symbols” really mean? Ultimately, it means two things: having sets of symbols (essentially just patterns that stand for things) to represent information, and processing (manipulating) those symbols in a specific way, using something like algebra (or logic, or computer programs) to operate over those symbols.
This article is as interesting for its exposition of symbolic approaches (relatively unknown to me) as for its sociology of science. Embraced by early computer scientists such as von Nuemann, symbolic approaches became dominant in the 1970s after intradisciplinary fighting between the symbolic camp and the neural network camp. Neural nets regained prominance in the 1980s, advanced by researchers who avoided the symbolic approach. The schism remains until today: Marcus’s polemic is itself is a testament to the rift. These are competing paradigms, though there are glimmers of a synthesis on the horizon.">



    
    
    

</head><body><div class="sidebar animated fadeInDown">
    <div class="logo-title">
      <div class="title">
        <img src="https://williammarble.com/img/headshot2023.jpeg" alt="profile picture">
        <h3 title=""><a href="/">William Marble</a></h3>
        <div class="description">
          
          
          Political Scientist<br>University of Pennsylvania
        </div>
      </div>
    </div>
    <ul class="social-links">
        
        <li>
        <a href="mailto:wpmarble@stanford.edu" rel="me" aria-label="e-mail">
          <i class="fas fa-lg fa-envelope" aria-hidden="true"></i>
        </a>          
        </li>

        
        <li>
        <a href="https://scholar.google.com/citations?user=3f1ym5MAAAAJ" rel="me" aria-label="google-scholar">
          <i class="fas fa-lg fa-graduation-cap" aria-hidden="true"></i>
        </a>          
        </li>

        
        <li>
        <a href="https://github.com/wpmarble/" rel="me" aria-label="GitHub">
          <i class="fab fa-lg fa-github" aria-hidden="true"></i>
        </a>          
        </li>

        
        <li>
        <a href="https://www.twitter.com/wpmarble" rel="me" aria-label="twitter">
          <i class="fab fa-lg fa-twitter" aria-hidden="true"></i>
        </a>          
        </li>

        
        <li>
        <a href="https://bsky.app/profile/wpmarble.bsky.social" rel="me" aria-label="BlueSky">
          <i class="fas fa-regular fa-cloud" aria-hidden="true"></i>
        </a>          
        </li>

        
    </ul>
    
    </div>
</div><div class="main">
            <div class="page-top animated fadeInDown">
    <a role="button" class="navbar-burger" data-target="navMenu" aria-label="menu" aria-expanded="false" >
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    <ul class="nav" id="navMenu">
        
        
        
        <li><a  href="/"  title="">Home</a></li>
        
        
        <li><a  href="/research/"  title="">Research</a></li>
        
        
        <li><a  href="/teaching/"  title="">Teaching</a></li>
        
        
        <li><a  href="/cv/"  title="">CV</a></li>
        
        
        <li><a  href="/posts/"  title="">Posts</a></li>
        
        
        <li class="theme-switch-item">
        <a class="theme-switch" title="Switch Theme">
            <i class="fa fa-adjust fa-fw" aria-hidden="true"></i>
        </a>
        </li>
    </ul>
</div>

            <div class="autopagerize_page_element">
                <div class="content">
<div class="post">
    <div class="post-content animated fadeInDown">

      <div class="post-title">
        <h3>Deep Learning and Knowledge Representation
        </h3>
        
        <div class="info">
          <i class="fa fa-edit"></i><span itemprop="name">William Marble</span><br>
          <i class="fa fa-calendar"></i><span class="date">Tue, Apr 5, 2022</span><br>
          <i class="fa fa-clock-o"></i><span class="reading-time">4-minute read</span>
        </div>
        
        </div>

    <p>In &ldquo;<a href="https://nautil.us/deep-learning-is-hitting-a-wall-14467/">Deep Learning is Hitting A Wall</a>,&rdquo; Gary Marcus argues that deep learning is reaching its limits as a paradigm for artificial intelligence. Recent difficulties with self-driving cars and unreliable language model outputs illustrate the limitations of systems that cannot <em>understand</em> the world, or the meaning of the words they are <a href="https://dl.acm.org/doi/10.1145/3442188.3445922">parroting</a>. Instead, Marcus advocates that field of AI turn its attention to symbolic approaches. Such approaches would focus on encoding information about the world and deriving information using simple operations:</p>
<blockquote>
<p>What does &ldquo;manipulating symbols&rdquo; really mean? Ultimately, it means two things: having sets of symbols (essentially just patterns that stand for things) to represent information, and processing (manipulating) those symbols in a specific way, using something like algebra (or logic, or computer programs) to operate over those symbols.</p>
</blockquote>
<p>This article is as interesting for its exposition of symbolic approaches (relatively unknown to me) as for its sociology of science. Embraced by early computer scientists such as von Nuemann, symbolic approaches became dominant in the 1970s after intradisciplinary fighting between the symbolic camp and the neural network camp. Neural nets regained prominance in the 1980s, advanced by researchers who avoided the symbolic approach. The schism remains until today: Marcus&rsquo;s polemic is itself is a testament to the rift. These are competing paradigms, though there are glimmers of a synthesis on the horizon.</p>
<p>One argument Marcus makes for symbolic approaches regards interpretability:</p>
<blockquote>
<p>Deep learning systems are black boxes; we can look at their inputs, and their outputs, but we have a lot of trouble peering inside. We don’t know exactly why they make the decisions they do, and often don’t know what to do about them (except to gather more data) if they come up with the wrong answers. This makes them inherently unwieldy and uninterpretable, and in many ways unsuited for “augmented cognition” in conjunction with humans. Hybrids that allow us to connect the learning prowess of deep learning, with the explicit, semantic richness of symbols, could be transformative.</p>
</blockquote>
<p>I find this argument persuasive. If the goal is to create a computer that can reason like humans, then we should learn from the way we, ourselves, organize knowledge.</p>
<p>Scientific investigation does not involve merely recording data about the world. Scientists seek to   <em>represent</em> data through the use of parsimonious, interpretable rules that are (mostly) mutually consistent with rules summarizing other data. That is, we create theories &mdash; abstract rules that explain  observations about how the world works. Theories have varying degrees of structure &mdash; Newton&rsquo;s laws are simpler than Einstein&rsquo;s theory.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>  In social science, we have theories that explain the relationships between variables in the social world &mdash; for example, Meltzer and Richard&rsquo;s model of <a href="https://www.jstor.org/stable/1830813">inequality and redistribution</a>.<!-- [^2 Meltzer and Richard are on the mind because I recently participated in a great conference on the spatial dimensions of inequality. ] --> When assessing new situations that are unlike those we have seen before, the theory guides us on what to expect. When the theory&rsquo;s predictions don&rsquo;t match the data, we update the theory not at random, but based on our knowledge of the world grounded in other theories. It stands to reason that artificial intelligence should make use of this method of representation.</p>
<p>These reflections also suggest an answer to a question I discussed with Matt Tyler early in grad school: If you had a black-box algorithm that could perfectly predict, say, how legislators would vote, could it be published in the <em>American Political Science Review</em>? My intuition is that most political scientists would not find such a paper interesting or important. The reason is that the way the algorithm represents knowledge &mdash; e.g., through a deep neural network &mdash; is not compatible with the way that political scientists represent knowledge.</p>
<p>Perhaps algebraic methods would help change my answer to this question. I&rsquo;m not very familiar with qualitative methods, but maybe something like qualitative comparative analysis, or quantitative variants like <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3639664">Bayesian rule sets</a>, could be helpful for theory generation. These methods seek to explain relationships in data through algebraic statements (e.g., IF high inequality THEN high redistribution). Relationships represented in this way may be more easily integrated into our existing mental schema than statistical relationships.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></p>
<p>We could then seek to develop theory that accounts for those patterns. Hopefully, the theory provides novel predictions that can be further tested against the data. And so on. In other words, maybe we should be investing in symbolic methods for theory development, and traditional statistical methods for testing.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>A working definition of the &ldquo;simplicity&rdquo; of theory might be the number of free parameters.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>A similar argument can be made for tree-based methods.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
    </div>
    <div class="post-footer animated fadeInDown">
      
      <div class="info">
        categories: 
<span class="separator"><a class="category" href="/categories/deep-learning/">deep-learning</a><a class="category" href="/categories/artificial-intelligence/">artificial-intelligence</a><a class="category" href="/categories/research-methods/">research-methods</a></span>

        
      </div>
      
    </div>

    
</div>


                </div>
            </div>
        </div>
</body>



<script type="text/javascript" src="https://williammarble.com/js/jquery.min.0e42e9ba16862440e6b0f0b86c778f992de98a45ca416acc1e80574bc6700efd.js" integrity="sha256-DkLpuhaGJEDmsPC4bHePmS3pikXKQWrMHoBXS8ZwDv0=" crossorigin="anonymous"></script>




<script type="text/javascript" src="https://williammarble.com/js/bundle.min.3ebad6bf39b2e7c2f899d3ed9bc97e8c0c1ead7a1b57472e3387321f673b8077.js" integrity="sha256-PrrWvzmy58L4mdPtm8l&#43;jAwerXobV0cuM4cyH2c7gHc=" crossorigin="anonymous"></script>

<script type="text/javascript" src="https://williammarble.com/js/medium-zoom.min.e1c6918cbaa90022a5612f0bd71c7bf3be6d036614c5729cebfe14f7b91fa4bc.js" integrity="sha256-4caRjLqpACKlYS8L1xx7875tA2YUxXKc6/4U97kfpLw=" crossorigin="anonymous"></script><script defer type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-e/4/LvThKH1gwzXhdbY2AsjR3rm7LHWyhIG5C0jiRfn8AN2eTN5ILeztWw0H9jmN" crossorigin="anonymous"></script>
        <script
            type="text/x-mathjax-config">MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });</script></html></body>

</html>
